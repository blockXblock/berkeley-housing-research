{"Berkeley-Data-Assets":{"slug":"Berkeley-Data-Assets","filePath":"Berkeley Data Assets.md","title":"Berkeley Data Assets","links":[],"tags":[],"content":"Berkeley already publishes most of the ingredients for a structure‚Äëby‚Äëstructure ‚Äúdigital twin‚Äù; the work is to harvest, normalize, and join them into a parcel‚Äëcentric knowledge graph keyed by APN and address.[1][2][3]\nOverall ingestion and data model\n\nAnchor everything on a master parcels table (one row per parcel) with APN, address, zoning district, land use, building footprint, and geometry; this comes from the Berkeley Open Data parcel layer and Community GIS.[2][3]\nFor the digital twin, design a schema where each parcel links to: permits (building, zoning, fire), ZAB actions, use permits, inspections, plans, staff reports, and any external datasets (assessor data, census, UC housing inventories).[4]\n\nCore city systems and document sources\n\nZoning Adjustments Board (ZAB): Agendas, packets, minutes, and supplemental communications are published for each meeting and link to project‚Äëlevel staff reports and plans; the board‚Äôs page is the entry point for bulk scraping of ZAB PDFs and metadata.[5][6]\nUse permits: The Use Permit page describes required applications and points to the zoning permit application process; actual records and attachments live in Permits Online / Accela (module ‚ÄúZoning‚Äù) and in Building Eye as map‚Äëlinked items.[7][8][1]\nBuilding and other permits: BuildingEye and the Accela Citizen Portal / Permits Online expose searchable records and attached plans (PDF) for building, electrical, plumbing, mechanical, and fire permits since roughly the mid‚Äë1990s.[8][9][4]\n\nOpen datasets, GIS, and base maps\n\nOpen Data Portal: Hosts tabular and GIS datasets for parcels, zoning, land use, business licenses, some transportation and environmental layers, all downloadable as CSV, shapefile, or GeoJSON for ingestion into your database or geospatial stack.[10][2]\nCommunity GIS Portal: Web map front‚Äëend with layers for parcels, zoning, environmental constraints, transportation, and city facilities; some of these layers correspond to downloadable open‚Äëdata sets and provide authoritative geometries for the digital twin.[3]\nZoning dataset and official zoning map: The ‚ÄúZoning‚Äù GIS layer defines all zoning districts and is the authoritative source for zone boundaries; the Berkeley Municipal Code‚Äôs Official Zoning Map provides legal zoning designations for cross‚Äëchecking.[11][12]\n\nPermits, plans, and structural information\n\nPermits Online / Accela attachments: For each building or zoning permit record, attachments include architectural plans, structural calculations, energy compliance documentation, and inspection reports; these PDFs are accessible through Record Info ‚Üí Attachments.[9][8]\nBuildingEye: Provides an interactive map of planning and building permits; use it to discover records by location and to cross‚Äëwalk address, permit IDs, and parcel/APN before ingesting details from Accela.[1][4]\nHistoric and non‚Äëdigitized records: The city notes that some older permits and plans are not fully online and must be retrieved via in‚Äëperson records research or public records requests; these would need scanning and manual indexing to complete the digital twin.[13][4]\n\nZAB, use permit, and staff report corpus\n\nZAB meeting packets and supplemental communications: Each meeting packet aggregates staff reports, project plans, draft findings, conditions of approval, and public correspondence; these packets form a rich text corpus for project‚Äëlevel decisions and design evolution.[6][5]\nStaff guidance and application requirements: The Use Permit and Zoning Permits pages link to submittal requirement documents and guides that specify what drawings and reports applicants must provide, giving a template for what to expect in each project file.[14][7]\nResearch Zoning Permits page: The ‚ÄúResearch zoning permits and zone designations‚Äù page explains how to use BuildingEye and the three main databases (BuildingEye, Permits Online, and Community GIS) to locate permit and zoning information for any building or parcel.[1]\n\nExternal and academic data sources\n\nState and regional GIS repositories: The California Open Data portal and state GIS hubs host additional Berkeley datasets (e.g., statewide city boundaries, environmental layers) that can be layered onto the digital twin.[15][16]\nUC Berkeley Library GIS holdings: The Earth Sciences &amp; Map Library maintains parcel snapshots and other Bay Area GIS datasets that can enrich or validate city data, accessible at dedicated GIS workstations.[16]\nReal estate and assessor data: Public assessor sites and commercial platforms provide assessed values, building year, and sometimes unit counts keyed by APN, useful for augmenting the parcel‚Äëlevel schema you already envisioned.\n\nPractical ingestion and integration steps\n\nUse scripted crawlers to harvest ZAB agendas/packets, parse project tables inside each PDF to extract application numbers, addresses, and actions, then join those to permit IDs from Accela / BuildingEye and to APNs from the parcel dataset.[6][1]\nPull periodic exports (or API pulls where possible) from BuildingEye and Accela to build time‚Äëseries tables of permits, inspections, and statuses; standardize on APN and normalized address as join keys across all systems.[4][8]\nImport GIS layers (parcels, zoning, land use) into a spatial database (PostGIS or SpatiaLite) and perform spatial joins so every permit, ZAB action, and document is attached to both a parcel polygon and its zoning district; this becomes the spatial backbone of the digital twin.[3][11]\n\nThis plan yields a parcel‚Äëcentric corpus where each structure and parcel in Berkeley is linked to its full regulatory, design, and permit history, ready for visualization and simulation in a Berkeley digital twin.[2][1]\nSources\n[1] Research Zoning Permits and Zone Designations | City of Berkeley berkeleyca.gov/construction-development/land-use-development/research-zoning-permits-and-zone-designations\n[2] Open Data Portal - The City of Berkeley berkeleyca.gov/your-government/public-records/open-data-portal\n[3] Community GIS Portal berkeleyca.gov/city-services/community-gis-portal\n[4] Can I combine data from Alameda County parcel records, the city of Berkeley parcel records, and other open source housing records for Berkeley to establish how many housing units are on each parcel www.perplexity.ai/search/5ffd159f-f4ed-40aa-a2b0-6f1d7a497fec\n[5] Research Permit Records | City of Berkeley berkeleyca.gov/construction-development/permits-design-parameters/permit-process/research-permit-records\n[6] Zoning Adjustments Board - The City of Berkeley berkeleyca.gov/your-government/boards-commissions/zoning-adjustments-board\n[7] [PDF] ZONING ADJUSTMENTS BOARD - The City of Berkeley berkeleyca.gov/sites/default/files/legislative-body-meeting-agendas/2025-12-11_ZAB_Agenda_Linked.pdf\n[8] Use Permit | City of Berkeley berkeleyca.gov/construction-development/permits-design-parameters/permit-types/use-permit\n[9] Building Permits - City of Berkeley aca-prod.accela.com/BERKELEY/Cap/CapHome.aspx\n[10] Permits Online | City of Berkeley berkeleyca.gov/construction-development/permits-design-parameters/permit-process/permits-online\n[11] City of Berkeley Open Data - The City of Berkeley data.cityofberkeley.info\n[12] Zoning | Open Data | City of Berkeley data.cityofberkeley.info/dataset/Zoning/iknk-w4qw\n[13] Official Zoning Map - Berkeley Municipal Code berkeley.municipal.codes/BMC/OfficialZoningMap\n[14] Berkeley Permit History Information - Megan Micco - Megan Micco www.meganmicco.com/blog/berkeley-permit-history/\n[15] Zoning Permits | City of Berkeley berkeleyca.gov/construction-development/permits-design-parameters/permit-types/zoning-permits\n[16] City of Berkeley - Dataset - California Open Data - CA.gov data.ca.gov/dataset/city-of-berkeley\n[17] California &amp; Bay Area GIS Data - UC Berkeley Library guide guides.lib.berkeley.edu/gis/California\n[18] I need to build data schemas that allow a join on APN between different tables from different providers. Do existing real estate websites that show ownership of street addresses Also use APN numbers? What are large real estate websites www.perplexity.ai/search/1d07690e-7a21-4576-9a4d-331de7312800\n[19] 23.402.070 Zoning Adjustments Board - Berkeley Municipal Code berkeley.municipal.codes/BMC/23.402.070\n[20] Agenda Center - Berkeley Township www.berkeleytownship.org/AgendaCenter\n[21] [PDF] plan commission meeting minutes march 14, 2018 - city www.berkeleymo.us/egov/documents/1523661115_17777.pdf\n[22] [PDF] The Board of Library Trustees may act on any item on this agenda www.berkeleypubliclibrary.org/sites/default/files/files/inline/2010_07_14_bolt_packet.pdf\n[23] Here is the December meeting schedule! We hope to see you in ‚Ä¶ www.facebook.com/CityofCrownPointIN/posts/here-is-the-december-meeting-schedule-we-hope-to-see-you-in-person-at-city-hall-/1263473242489104/\n[24] SF PIM | Property Information Map | SF Planning sfplanninggis.org/pim/\n[25] Administrative Use Permit | City of Berkeley berkeleyca.gov/construction-development/permits-design-parameters/permit-types/administrative-use-permit"},"about/about":{"slug":"about/about","filePath":"about/about.md","title":"about","links":["Berkeley-Data-Assets"],"tags":[],"content":"Berkeley Housing Research\nFramework for Open Berkeley Data\n\n\nEvolve a general data framework for municipalities to open all aspects of their permitting pipeline for access by data science analysts, general data science instruction, and the general public\n\n\nHere‚Äôs a first-pass at building a suite of JN to create the Annual Public Report for every city in California. \n\n\nEvery city has a home-brew collection of databases, permit pipelines, spreadsheets, hacks and patches to satisfy the HCD annual report requirements\n\nHCD has a least-common-denominator workflow for city reports, which degrades the data quality to an almost unusable level.\nBerkeley uses Accela and Scala for open data access: terrible, does not allow bulk downloads of all permit or parcel data.  Can get 12,000 Berkeley business licenses, but totally inadequate to build city economic analysis of impact of new housing on city economy.\n\n\n\n\nSee overview of today‚Äôs City of Berkeley housing-related  data\n\nSadly outdated. Perhaps new Clariti contract can be re-negotiated\nBerkeley Data Assets\n\n\n\nIllustrative Data Elements for Housing\n\nall relevant ordinances implementing California Housing Community Develeopment guidelines for annual municipal reporting\ntimestamped components of permitting or inspecting any aspect of municipal housing\nPermit pipeline metadata\n\nallow analyis of progress of individual permit requests through the various stages of the pipeline\nshow references to the law cited as foundation of the permit request\n\nAB2011\nSB79\nSB330\n\n\n\n\nUse Terner Center database of all California Legislation for updates\n\nSadly, it‚Äôs incomplete, not kept up to date\nReplace it with NotebookLM\nExport to Datasette SQL open db\n\n\n"},"documentation/Data-Loading-Code":{"slug":"documentation/Data-Loading-Code","filePath":"documentation/Data-Loading-Code.md","title":"Data-Loading-Code","links":["tags/code","tags/data-loading","tags/examples"],"tags":["code","data-loading","examples"],"content":"Data Loading Code Snippets\nTags: code data-loading examples\n\nüìä Loading from Different Sources\n1. CSV Files\nimport pandas as pd\n \n# Basic CSV load\ndf = pd.read_csv(&#039;housing_projects.csv&#039;)\n \n# With encoding specification\ndf = pd.read_csv(&#039;permits.csv&#039;, encoding=&#039;utf-8&#039;)\n \n# With specific dtypes\ndf = pd.read_csv(&#039;permits.csv&#039;, dtype={\n    &#039;net_units&#039;: int,\n    &#039;year&#039;: int,\n    &#039;latitude&#039;: float,\n    &#039;longitude&#039;: float\n})\n \n# Skip bad lines\ndf = pd.read_csv(&#039;permits.csv&#039;, on_bad_lines=&#039;skip&#039;)\n \n# Parse dates\ndf = pd.read_csv(&#039;permits.csv&#039;, parse_dates=[&#039;permit_date&#039;])\n\n2. Excel Files (.xlsx, .xls)\nimport pandas as pd\n \n# Read first sheet\ndf = pd.read_excel(&#039;building_permits.xlsx&#039;)\n \n# Read specific sheet\ndf = pd.read_excel(&#039;building_permits.xlsx&#039;, sheet_name=&#039;Permits 2024&#039;)\n \n# Read all sheets\nexcel_file = pd.ExcelFile(&#039;building_permits.xlsx&#039;)\nprint(f&quot;Available sheets: {excel_file.sheet_names}&quot;)\n \nsheets = {}\nfor sheet_name in excel_file.sheet_names:\n    sheets[sheet_name] = pd.read_excel(excel_file, sheet_name=sheet_name)\n \n# Combine multiple sheets\nall_data = pd.concat(sheets.values(), ignore_index=True)\n \n# Skip header rows\ndf = pd.read_excel(&#039;permits.xlsx&#039;, skiprows=3)\n \n# Specify columns\ndf = pd.read_excel(&#039;permits.xlsx&#039;, usecols=[&#039;Address&#039;, &#039;Units&#039;, &#039;Year&#039;])\n\n3. Google Sheets\nimport pandas as pd\nimport gspread\nfrom oauth2client.service_account import ServiceAccountCredentials\n \n# Method 1: Using gspread (requires API credentials)\nscope = [&#039;spreadsheets.google.com/feeds&#039;,\n         &#039;www.googleapis.com/auth/drive&#039;]\n \ncredentials = ServiceAccountCredentials.from_json_keyfile_name(\n    &#039;credentials.json&#039;, scope)\nclient = gspread.authorize(credentials)\n \n# Open sheet\nsheet = client.open(&#039;Berkeley Building Permits&#039;).sheet1\ndata = sheet.get_all_records()\ndf = pd.DataFrame(data)\n \n# Method 2: Public sheet via URL\nsheet_url = &#039;docs.google.com/spreadsheets/d/YOUR_SHEET_ID/export#039;\ndf = pd.read_csv(sheet_url)\n \n# Method 3: Using sheet ID\nsheet_id = &#039;YOUR_SHEET_ID&#039;\ngid = &#039;0&#039;  # Sheet tab ID\nurl = f&#039;docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&amp;gid={gid}&#039;\ndf = pd.read_csv(url)\n\n4. PDF Files (Table Extraction)\nimport pandas as pd\nimport tabula\nimport pdfplumber\n \n# Method 1: Using tabula-py (Java-based)\n# pip install tabula-py\n \n# Extract all tables from PDF\ntables = tabula.read_pdf(&#039;building_permits.pdf&#039;, pages=&#039;all&#039;)\n \n# Extract from specific page\ntables = tabula.read_pdf(&#039;building_permits.pdf&#039;, pages=1)\n \n# Extract with specific area (coordinates)\ntables = tabula.read_pdf(\n    &#039;building_permits.pdf&#039;,\n    area=[50, 0, 800, 600],  # [top, left, bottom, right]\n    pages=1\n)\n \n# Combine all tables\ndf = pd.concat(tables, ignore_index=True)\n \n# Method 2: Using pdfplumber (Python-based, more flexible)\n# pip install pdfplumber\n \nwith pdfplumber.open(&#039;building_permits.pdf&#039;) as pdf:\n    tables = []\n    \n    for page in pdf.pages:\n        # Extract tables from page\n        page_tables = page.extract_tables()\n        \n        for table in page_tables:\n            # Convert to DataFrame\n            if table:\n                df_page = pd.DataFrame(table[1:], columns=table[0])\n                tables.append(df_page)\n    \n    # Combine all tables\n    df = pd.concat(tables, ignore_index=True)\n \n# Method 3: Extract text and parse manually\nwith pdfplumber.open(&#039;building_permits.pdf&#039;) as pdf:\n    text = &#039;&#039;\n    for page in pdf.pages:\n        text += page.extract_text()\n    \n    # Parse text (custom logic needed)\n    lines = text.split(&#039;\\n&#039;)\n    # Process lines...\n\n5. PDF Files (Text Extraction with Patterns)\nimport re\nimport pdfplumber\nimport pandas as pd\n \ndef extract_permits_from_pdf(pdf_path):\n    &quot;&quot;&quot;\n    Extract building permit data from PDF using regex patterns\n    &quot;&quot;&quot;\n    \n    permits = []\n    \n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text = page.extract_text()\n            \n            # Example pattern: &quot;Address: 123 Main St, Units: 50&quot;\n            pattern = r&#039;Address:\\s*([^,]+),\\s*Units:\\s*(\\d+)&#039;\n            \n            for match in re.finditer(pattern, text):\n                address = match.group(1).strip()\n                units = int(match.group(2))\n                \n                permits.append({\n                    &#039;address&#039;: address,\n                    &#039;net_units&#039;: units\n                })\n    \n    return pd.DataFrame(permits)\n \n# Usage\ndf = extract_permits_from_pdf(&#039;building_permits.pdf&#039;)\nprint(f&quot;Extracted {len(df)} permits from PDF&quot;)\n\n6. Combining Multiple Files\nfrom pathlib import Path\nimport pandas as pd\n \n# Load all CSVs from directory\ndata_dir = Path(&#039;permit_data&#039;)\ncsv_files = data_dir.glob(&#039;*.csv&#039;)\n \ndfs = []\nfor csv_file in csv_files:\n    print(f&quot;Loading: {csv_file.name}&quot;)\n    df = pd.read_csv(csv_file)\n    df[&#039;source_file&#039;] = csv_file.name\n    dfs.append(df)\n \n# Combine\ndf_combined = pd.concat(dfs, ignore_index=True)\nprint(f&quot;Total rows: {len(df_combined)}&quot;)\n\n7. API Data Retrieval (Socrata)\nimport requests\nimport pandas as pd\nimport time\n \ndef download_socrata_data(domain, dataset_id, app_token=None, limit=None):\n    &quot;&quot;&quot;\n    Download data from Socrata open data portal\n    \n    Args:\n        domain: e.g., &quot;data.cityofberkeley.info&quot;\n        dataset_id: e.g., &quot;ydr8-5enu&quot;\n        app_token: Optional API token\n        limit: Max rows to retrieve\n    &quot;&quot;&quot;\n    \n    base_url = f&quot;https://{domain}/resource/{dataset_id}.json&quot;\n    \n    headers = {}\n    if app_token:\n        headers[&#039;X-App-Token&#039;] = app_token\n    \n    all_data = []\n    offset = 0\n    page_size = 1000\n    \n    while True:\n        params = {\n            &#039;$limit&#039;: page_size,\n            &#039;$offset&#039;: offset\n        }\n        \n        response = requests.get(base_url, headers=headers, params=params, timeout=30)\n        \n        if response.status_code != 200:\n            print(f&quot;Error {response.status_code}: {response.text}&quot;)\n            break\n        \n        data = response.json()\n        \n        if not data:\n            break\n        \n        all_data.extend(data)\n        print(f&quot;Downloaded {len(all_data)} rows...&quot;)\n        \n        if limit and len(all_data) &gt;= limit:\n            all_data = all_data[:limit]\n            break\n        \n        if len(data) &lt; page_size:\n            break\n        \n        offset += page_size\n        time.sleep(0.5)  # Be nice to API\n    \n    return pd.DataFrame(all_data)\n \n# Usage\ndf = download_socrata_data(\n    domain=&#039;data.cityofberkeley.info&#039;,\n    dataset_id=&#039;ydr8-5enu&#039;,\n    app_token=&#039;YOUR_TOKEN&#039;,\n    limit=5000\n)\n\n8. Accela API (Permit Systems)\nimport requests\nimport pandas as pd\n \ndef download_from_accela(base_url, api_key, module=&#039;Building&#039;, record_type=&#039;Permit&#039;):\n    &quot;&quot;&quot;\n    Download permits from Accela-based system\n    \n    Many cities use Accela for permit tracking\n    &quot;&quot;&quot;\n    \n    headers = {\n        &#039;x-accela-appid&#039;: api_key,\n        &#039;Content-Type&#039;: &#039;application/json&#039;\n    }\n    \n    params = {\n        &#039;module&#039;: module,\n        &#039;type&#039;: record_type,\n        &#039;limit&#039;: 1000,\n        &#039;offset&#039;: 0\n    }\n    \n    all_records = []\n    \n    while True:\n        response = requests.get(\n            f&quot;{base_url}/v4/records&quot;,\n            headers=headers,\n            params=params\n        )\n        \n        if response.status_code != 200:\n            print(f&quot;Error: {response.status_code}&quot;)\n            break\n        \n        data = response.json()\n        records = data.get(&#039;result&#039;, [])\n        \n        if not records:\n            break\n        \n        all_records.extend(records)\n        print(f&quot;Downloaded {len(all_records)} records...&quot;)\n        \n        params[&#039;offset&#039;] += params[&#039;limit&#039;]\n        time.sleep(1)\n    \n    return pd.DataFrame(all_records)\n\n9. Handling Common Issues\nimport pandas as pd\nimport chardet\n \n# Detect encoding\nwith open(&#039;building_permits.csv&#039;, &#039;rb&#039;) as f:\n    result = chardet.detect(f.read())\n    encoding = result[&#039;encoding&#039;]\n \nprint(f&quot;Detected encoding: {encoding}&quot;)\ndf = pd.read_csv(&#039;building_permits.csv&#039;, encoding=encoding)\n \n# Handle different date formats\ndf = pd.read_csv(&#039;permits.csv&#039;, parse_dates=[&#039;permit_date&#039;])\n \n# If date parsing fails, convert manually\ndf[&#039;permit_date&#039;] = pd.to_datetime(df[&#039;permit_date&#039;], format=&#039;%m/%d/%Y&#039;, errors=&#039;coerce&#039;)\n \n# Handle missing values\ndf = pd.read_csv(&#039;permits.csv&#039;, na_values=[&#039;N/A&#039;, &#039;NA&#039;, &#039;&#039;, &#039;None&#039;, &#039;null&#039;])\n \n# Handle thousands separator\ndf = pd.read_csv(&#039;permits.csv&#039;, thousands=&#039;,&#039;)\n \n# Handle different delimiters\ndf = pd.read_csv(&#039;permits.txt&#039;, sep=&#039;\\t&#039;)  # Tab-separated\ndf = pd.read_csv(&#039;permits.txt&#039;, sep=&#039;|&#039;)   # Pipe-separated\n\n10. Validate Loaded Data\ndef validate_permit_data(df):\n    &quot;&quot;&quot;Validate loaded permit data&quot;&quot;&quot;\n    \n    print(&quot;üîç VALIDATING DATA&quot;)\n    print(&quot;=&quot;*70)\n    \n    # Check required columns\n    required = [&#039;address&#039;, &#039;net_units&#039;, &#039;year&#039;]\n    missing = set(required) - set(df.columns)\n    \n    if missing:\n        print(f&quot;‚ùå Missing columns: {missing}&quot;)\n        return False\n    else:\n        print(f&quot;‚úÖ All required columns present&quot;)\n    \n    # Check row count\n    print(f&quot;   Rows: {len(df):,}&quot;)\n    \n    # Check for nulls\n    null_counts = df[required].isnull().sum()\n    if null_counts.any():\n        print(f&quot;‚ö†Ô∏è  Null values found:&quot;)\n        for col in null_counts[null_counts &gt; 0].index:\n            print(f&quot;   {col}: {null_counts[col]} nulls&quot;)\n    else:\n        print(f&quot;‚úÖ No null values in required columns&quot;)\n    \n    # Check data types\n    if pd.api.types.is_numeric_dtype(df[&#039;net_units&#039;]):\n        print(f&quot;‚úÖ net_units is numeric&quot;)\n    else:\n        print(f&quot;‚ö†Ô∏è  net_units is not numeric: {df[&#039;net_units&#039;].dtype}&quot;)\n    \n    # Check value ranges\n    if (df[&#039;net_units&#039;] &lt;= 0).any():\n        print(f&quot;‚ö†Ô∏è  Found {(df[&#039;net_units&#039;] &lt;= 0).sum()} rows with units &lt;= 0&quot;)\n    \n    print(&quot;=&quot;*70)\n    return True\n \n# Usage\ndf = pd.read_csv(&#039;building_permits.csv&#039;)\nvalidate_permit_data(df)\n\nLast Updated: 2026-01-09"},"documentation/Database-Creation-Code":{"slug":"documentation/Database-Creation-Code","filePath":"documentation/Database-Creation-Code.md","title":"Database-Creation-Code","links":["tags/code","tags/sqlite"],"tags":["code","sqlite"],"content":"Database Creation Code\nTags: code sqlite\nComplete Script\n#!/usr/bin/env python3\nimport pandas as pd\nimport sqlite_utils\n \n# Load CSV\ndf = pd.read_csv(&#039;housing_projects_final_complete.csv&#039;)\nprint(f&quot;Loaded {len(df)} projects&quot;)\n \n# Create database\ndb = sqlite_utils.Database(&#039;berkeley_housing.db&#039;)\ndb[&#039;projects&#039;].insert_all(df.to_dict(&#039;records&#039;), replace=True)\n \n# Create indexes\ndb[&#039;projects&#039;].create_index([&#039;year&#039;])\ndb[&#039;projects&#039;].create_index([&#039;net_units&#039;])\ndb[&#039;projects&#039;].create_index([&#039;latitude&#039;, &#039;longitude&#039;])\n \n# Create map view\ndb.execute(&quot;&quot;&quot;\nCREATE VIEW IF NOT EXISTS map_view AS\nSELECT \n    address_display as address,\n    net_units, year, status, street,\n    latitude, longitude,\n    CASE \n        WHEN net_units &gt;= 200 THEN &#039;üî¥ 200+ units&#039;\n        WHEN net_units &gt;= 100 THEN &#039;üü† 100-199 units&#039;\n        WHEN net_units &gt;= 50 THEN &#039;üîµ 50-99 units&#039;\n        WHEN net_units &gt;= 20 THEN &#039;üü¢ 20-49 units&#039;\n        ELSE &#039;üî∑ &lt;20 units&#039;\n    END as size_category\nFROM projects\nWHERE latitude IS NOT NULL\n&quot;&quot;&quot;)\n \nprint(&quot;‚úÖ Database created!&quot;)\nValidate Database\nimport sqlite3\n \nconn = sqlite3.connect(&#039;berkeley_housing.db&#039;)\ncursor = conn.cursor()\n \n# Check row counts\nprojects_count = cursor.execute(&quot;SELECT COUNT(*) FROM projects&quot;).fetchone()[0]\nmap_view_count = cursor.execute(&quot;SELECT COUNT(*) FROM map_view&quot;).fetchone()[0]\n \nprint(f&quot;projects: {projects_count} rows&quot;)\nprint(f&quot;map_view: {map_view_count} rows&quot;)\nLast Updated: 2026-01-09"},"documentation/Difficult-Data-Sources":{"slug":"documentation/Difficult-Data-Sources","filePath":"documentation/Difficult-Data-Sources.md","title":"Difficult-Data-Sources","links":["tags/workarounds","tags/education"],"tags":["workarounds","education"],"content":"Working with Difficult Data Sources\nTags: workarounds education\nCommon Obstacles\n1. API Access Blocked (403 Forbidden)\nBerkeley‚Äôs case:\nresponse = requests.get(&#039;data.cityofberkeley.info/resource/ydr8-5enu.json&#039;)\nprint(response.status_code)  # 403 Forbidden\nWorkaround: Manual Download\n1. Visit dataset page\n2. Click &quot;Export&quot; ‚Üí &quot;CSV&quot;\n3. Save to project directory\n4. Document process in README\n2. No API Available\nSolution: Web Scraping (Ethical)\nimport requests\nfrom bs4 import BeautifulSoup\n \nheaders = {&#039;User-Agent&#039;: &#039;Research Project (contact@example.com)&#039;}\nresponse = requests.get(url, headers=headers)\n# Parse HTML...\ntime.sleep(2)  # Be polite\n3. Freedom of Information Request\nTemplate:\nSubject: Public Records Request - Building Permit Data\n\nI am requesting access to building permit records for \neducational research purposes...\n\nFor Students\nKey lesson: Real civic data work requires:\n\nTechnical skills (APIs, scraping)\nProblem-solving (workarounds)\nCommunication (working with officials)\nPersistence (trying multiple approaches)\n\nLast Updated: 2026-01-09"},"documentation/Map-Queries-Code":{"slug":"documentation/Map-Queries-Code","filePath":"documentation/Map-Queries-Code.md","title":"Map-Queries-Code","links":["tags/code","tags/sql","tags/maps"],"tags":["code","sql","maps"],"content":"Map Queries - SQL Code\nTags: code sql maps\nQuery 1: By Size Category\nSELECT \n    address, net_units, year, \n    latitude, longitude, size_category\nFROM map_view\nORDER BY net_units DESC;\nQuery 2: Recent Projects (2020+)\nSELECT \n    address, net_units, year,\n    latitude, longitude,\n    year || &#039; | &#039; || address || &#039; | &#039; || net_units || &#039; units&#039; as popup_label\nFROM map_view\nWHERE year &gt;= 2020\nORDER BY year DESC, net_units DESC;\nQuery 3: Geographic Clustering\nWITH project_clusters AS (\n    SELECT \n        p1.address, p1.latitude, p1.longitude,\n        p1.net_units, p1.year,\n        COUNT(DISTINCT p2.address) - 1 as nearby_projects\n    FROM map_view p1\n    LEFT JOIN map_view p2 \n        ON ABS(p1.latitude - p2.latitude) &lt; 0.005\n        AND ABS(p1.longitude - p2.longitude) &lt; 0.005\n    GROUP BY p1.address, p1.latitude, p1.longitude, p1.net_units, p1.year\n)\nSELECT \n    address, latitude, longitude, net_units, year,\n    nearby_projects,\n    CASE \n        WHEN nearby_projects &gt;= 10 THEN &#039;üèôÔ∏è High Density&#039;\n        WHEN nearby_projects &gt;= 5 THEN &#039;üèòÔ∏è Cluster&#039;\n        WHEN nearby_projects &gt;= 2 THEN &#039;üèóÔ∏è Multiple&#039;\n        ELSE &#039;üè† Isolated&#039;\n    END as cluster_type\nFROM project_clusters\nORDER BY nearby_projects DESC;\nQuery 4: Top 20 Largest\nSELECT \n    address, net_units, year,\n    latitude, longitude,\n    ROW_NUMBER() OVER (ORDER BY net_units DESC) as rank\nFROM map_view\nORDER BY net_units DESC\nLIMIT 20;\nLast Updated: 2026-01-09"},"documentation/Test-Scripts-Code":{"slug":"documentation/Test-Scripts-Code","filePath":"documentation/Test-Scripts-Code.md","title":"Test-Scripts-Code","links":["tags/code","tags/testing"],"tags":["code","testing"],"content":"Test Scripts Code\nTags: code testing\nrun_test.sh - Interactive Testing\n#!/bin/bash\n \nTEST_BASE=&quot;$HOME/berkeley-housing-test&quot;\nTEST_DATE=$(date +%Y%m%d_%H%M%S)\nTEST_DIR=&quot;$TEST_BASE/test-runs/run_${TEST_DATE}&quot;\n \necho &quot;üß™ Creating test environment: $TEST_DIR&quot;\n \n# Create structure\nmkdir -p &quot;$TEST_DIR&quot;/{inputs,outputs,temp}\n \n# Copy notebook\ncp ~/berkeley-data/berkeley_open_data_pipeline.ipynb &quot;$TEST_DIR/&quot;\n \n# Copy inputs\ncp ~/berkeley-data/alameda_lookup_complete.csv &quot;$TEST_DIR/inputs/&quot;\n \n# Open Jupyter\ncd &quot;$TEST_DIR&quot;\njupyter notebook\ntest_headless.sh - Automated Testing\n#!/bin/bash\n \nTEST_DIR=&quot;$HOME/berkeley-housing-test/test-runs/run_$(date +%Y%m%d_%H%M%S)&quot;\n \nmkdir -p &quot;$TEST_DIR&quot;/{inputs,outputs,temp}\ncd &quot;$TEST_DIR&quot;\n \n# Copy files\ncp ~/berkeley-data/berkeley_open_data_pipeline.ipynb ./notebook.ipynb\ncp ~/berkeley-data/alameda_lookup_complete.csv inputs/\n \n# Execute\njupyter nbconvert \\\n    --to notebook \\\n    --execute \\\n    --ExecutePreprocessor.timeout=600 \\\n    notebook.ipynb\n \nif [ $? -eq 0 ]; then\n    echo &quot;‚úÖ Test passed&quot;\n    ls -lh outputs/\nelse\n    echo &quot;‚ùå Test failed&quot;\nfi\ncleanup_old_tests.sh\n#!/bin/bash\n \ncd ~/berkeley-housing-test/test-runs\n \n# Find tests older than 7 days\nOLD_TESTS=$(find . -maxdepth 1 -type d -mtime +7 -name &quot;run_*&quot;)\n \nif [ -z &quot;$OLD_TESTS&quot; ]; then\n    echo &quot;No old tests to clean&quot;\nelse\n    echo &quot;Old tests:&quot;\n    echo &quot;$OLD_TESTS&quot;\n    read -p &quot;Delete? (yes/no): &quot; CONFIRM\n    \n    if [ &quot;$CONFIRM&quot; = &quot;yes&quot; ]; then\n        echo &quot;$OLD_TESTS&quot; | xargs rm -rf\n        echo &quot;‚úÖ Cleaned up&quot;\n    fi\nfi\nLast Updated: 2026-01-09"},"index":{"slug":"index","filePath":"index.md","title":"Berkeley Housing Data Science Platform","links":["research/clariti-research","research/api-requirements","about/about","notebooks"],"tags":[],"content":"Berkeley Housing Data Science\n\nAnalyzing housing development and permits in Berkeley, California.\n\nResearch\n\nClariti Research\nAPI Requirements\n\nInteractive Tools\n\nLive Database\nGitHub Repository\n\nAbout\n\nAbout This Project\n\nüíª Computational Notebooks\nExplore our collection of 12+ Jupyter notebooks:\n\nBrowse All Notebooks\nLaunch in Browser (Binder)\nView on GitHub\n"},"notebooks/index":{"slug":"notebooks/index","filePath":"notebooks/index.md","title":"Jupyter Notebooks","links":["documentation"],"tags":[],"content":"Computational Notebooks\nInteractive Jupyter notebooks for Berkeley housing analysis.\nüöÄ Quick Start\n\nClick above to launch all notebooks in your browser (no installation required!)\n\nüìä Workflow Notebooks\nA. Data Collection &amp; Processing\n\n\nA1: Data Sources Setup\n\nConnect to Berkeley Open Data\nAPI configuration\nDataset discovery\n\n\n\nA2: Address Standardization\n\nClean address data\nStandardize formats\nHandle variations\n\n\n\nA3: Geocoding Pipeline\n\nConvert addresses to coordinates\n100% geocoding success\nAlameda County lookup integration\n\n\n\nB. Timeline Tracking\n\n\nB1: Lifecycle Tracking\n\nTrack project from proposal to completion\nIdentify bottlenecks\nTimeline visualization\n\n\n\nB2: Status Classification\n\nCategorize project status\nStandardize status codes\nProgress tracking\n\n\n\nB3: Progress Indicators\n\nCalculate progress metrics\nIdentify stalled projects\nPredict completion dates\n\n\n\nC. Analysis\n\n\nC1: Pipeline Analysis\n\nAnalyze housing pipeline\nDevelopment trends\nGeographic patterns\n\n\n\nC2: Timeline Analysis\n\nReview process timelines\nApproval durations\nSeasonal patterns\n\n\n\nC3: Proposal vs Reality\n\nCompare proposed vs built units\nAnalyze changes during review\nOutcome assessment\n\n\n\nD. Reporting &amp; Monitoring\n\n\nD1: Monthly Report Generator\n\nGenerate automated reports\nTrack monthly progress\nExport visualizations\n\n\n\nD2: Dashboard Data Export\n\nPrepare data for dashboards\nCreate summary statistics\nExport for visualization tools\n\n\n\nD3: Alerts &amp; Monitoring\n\nMonitor for anomalies\nIdentify stalled projects\nGenerate alerts\n\n\n\n\nüéì Complete Analysis\nMASTER_ANALYSIS.ipynb\nComprehensive analysis combining all workflows. Best for:\n\nUnderstanding the complete pipeline\nReproducing all results\nLearning the full methodology\n\n\nüíª Running Locally\n# Clone repository\ngit clone github.com/blockXblock/berkeley-housing-analysis.git\ncd berkeley-housing-analysis\n \n# Install dependencies\npip install -r requirements.txt\n \n# Launch Jupyter\njupyter notebook\n\nüìö Learning Path\nBeginners: Start with A1 ‚Üí A2 ‚Üí A3\nIntermediate: Add B1 ‚Üí C1\nAdvanced: Explore full workflow A‚ÜíB‚ÜíC‚ÜíD\nResearchers: Use MASTER_ANALYSIS.ipynb\n\nüîó Resources\n\nLive Database\nDocumentation\nGitHub Repository\n"},"research/clariti-opengov-comparison/findings":{"slug":"research/clariti-opengov-comparison/findings","filePath":"research/clariti-opengov-comparison/findings.md","title":"findings","links":[],"tags":[],"content":"Clariti and OpenGov Implementation Research\nDate: 2026-01-14\n\nCLARITI RESEARCH\nCompany Background\nClariti (based in Canada)\n\nFull name: Clariti Inc.\nFocus: Municipal permit and land management software\nProduct suite: Permits, planning, licensing, bylaws\n\nKnown Implementations\n[To be researched via web search]\nSearch queries to run:\n\n‚ÄúClariti permit software cities using‚Äù\n‚ÄúClariti API documentation‚Äù\n‚ÄúClariti municipal implementations‚Äù\nsite:clariti.ca customers\n\n\nOPENGOV RESEARCH\nPermitSF (San Francisco)\nProject: PermitSF\nLaunch: 2024-2025 (recent)\nVendor: OpenGov\nPurpose: Replace legacy BIPS system\nKey Information Needed:\n\nDoes PermitSF have a public API?\nIntegration with SF‚Äôs DataSF portal?\nTimeline from selection to launch?\nPublic documentation?\n\nOpenGov Company\n\nMajor player in gov tech\nMultiple product lines (not just permits)\nMany US city clients\n\nSearch queries:\n\n‚ÄúPermitSF API‚Äù\n‚ÄúSan Francisco PermitSF documentation‚Äù\n‚ÄúDataSF permit data‚Äù\n‚ÄúOpenGov permit API‚Äù\nsite:sf.gov permitsf\n\n\nCOMPARABLE SYSTEMS TO RESEARCH\n1. Seattle (What do they use?)\n\nKnown for strong open data\nCheck: data.seattle.gov\n\n2. Los Angeles (LADBS)\n\nLarge permit volume\nCheck API availability\n\n3. Portland (BDS)\n\nProgressive open data policies\nCheck their implementation\n\n4. Austin\n\nSmart city initiatives\nOpen data portal\n\n\nQUESTIONS FOR BERKELEY STAFF\n\n\nWhy Clariti over OpenGov?\n\nCost comparison?\nFeature requirements?\nTimeline considerations?\n\n\n\nAPI Requirements in RFP?\n\nWas public API access required?\nWhat level of access was specified?\n\n\n\nImplementation Timeline\n\nWhen does migration start?\nWhen will Clariti be live?\nWill there be parallel access to Accela during transition?\n\n\n\nData Migration\n\nHow much history migrated?\nData quality concerns?\nAPI access during migration?\n\n\n\nWho are the decision makers?\n\nIT Director\nBuilding Official\nPlanning Director\nCIO\n\n\n\n\nADVOCACY STRATEGY\nImmediate Actions\n\nSubmit API requirements document\nRequest meeting with implementation team\nConnect with other researchers/civic tech groups\nMonitor SF‚Äôs PermitSF as comparison\n\nKey Stakeholders\n\nBerkeley IT Department\nPlanning &amp; Development Department\nCity Manager‚Äôs Office\nInnovation &amp; Technology Commission (if exists)\nUC Berkeley researchers\nCivic tech community (Open Oakland, etc.)\n\nMessaging Points\n\nAcademic research value\nGovernment transparency\nEconomic development insights\nPublic health applications\nClimate action data needs\nHousing crisis monitoring\n\n\nNEXT STEPS\n\nWeb research (using search tools)\nContact cities using these systems\nReview SF PermitSF implementation\nDraft advocacy letter to Berkeley\nEngage civic tech community\n"},"research/clariti-opengov-comparison/research_plan":{"slug":"research/clariti-opengov-comparison/research_plan","filePath":"research/clariti-opengov-comparison/research_plan.md","title":"research_plan","links":[],"tags":[],"content":"Clariti vs OpenGov Research Plan\nResearch Questions\nClariti (Berkeley‚Äôs Choice)\n\nWhat cities currently use Clariti?\nDo any have public APIs?\nWhat‚Äôs their data model?\nCanadian origin - any US implementations?\nWhat‚Äôs their track record?\n\nOpenGov (SF‚Äôs Choice - PermitSF)\n\nPermitSF implementation details\nSF‚Äôs API architecture\nPublic access to permit data\nIntegration with DataSF\nLessons learned we can apply to Berkeley\n\nComparison\n\nFeature matrix\nAPI capabilities\nCost (if public)\nImplementation timeline\nUser satisfaction\nDeveloper community\n\nSearch Strategy\n\nCity websites with ‚ÄúClariti‚Äù or ‚ÄúOpenGov‚Äù\nAPI documentation searches\nUrban planning forums\nGovernment procurement records\nAcademic papers using these systems\n"},"research/clariti/API_Requirements_Berkeley":{"slug":"research/clariti/API_Requirements_Berkeley","filePath":"research/clariti/API_Requirements_Berkeley.md","title":"API_Requirements_Berkeley","links":[],"tags":[],"content":"Clariti API Requirements for City of Berkeley\nCivic Data Science &amp; Smart City Analytics\nDate: 2026-01-14\nPrepared by: Berkeley Housing Data Science Project\nPurpose: Enable comprehensive urban analytics, research, and public transparency\n\n1. EXECUTIVE SUMMARY\nThe City of Berkeley is transitioning to Clariti for permit management. To support:\n\nAcademic research\nCivic transparency\nData-driven policy\nSmart city initiatives\nPublic health analysis\nEconomic development tracking\n\nWe require comprehensive, programmatic API access to all permit and planning data.\n\n2. CORE REQUIREMENTS\n2.1 API Architecture\nRESTful API with:\n\nJSON response format (primary)\nCSV export capability (for bulk downloads)\nGraphQL support (optional, for complex queries)\nWebSocket support (optional, for real-time updates)\n\nAuthentication:\n\nApp token system (like Socrata)\nOAuth 2.0 for authenticated users\nRate limiting that allows reasonable academic/research use\nHigher limits for registered .edu users\n\nBase URL Structure:\napi.clariti.berkeley.gov/v1/\ndata.cityofberkeley.info/clariti/v1/  (alternative)\n\n\n2.2 Essential Data Endpoints\nA. Building Permits\nGET /permits/building\nGET /permits/building/{permit_id}\nGET /permits/building/search?{filters}\n\nRequired Fields:\n\npermit_id (unique identifier)\npermit_number (display number)\naddress (full street address)\nparcel_apn (assessor parcel number)\npermit_type (new construction, addition, alteration, etc.)\nwork_description (detailed description)\nstatus (issued, under review, completed, etc.)\nstatus_date (date of current status)\nissue_date (when permit was issued)\nfinal_date (completion/CO date)\nexpiration_date\nvaluation (project value)\nunits_new (new housing units created)\nunits_demolished (units removed)\nsquare_footage (total square footage)\nstories (number of stories)\noccupancy_type\nconstruction_type\napplicant_name\ncontractor_name\narchitect_name\ncoordinates (latitude, longitude in WGS84/EPSG:4326)\n\nOptional but Valuable:\n\ninspection_history (array of inspections)\nfees_paid\nappeals (any appeals filed)\nrelated_permits (links to other permits)\ndocuments (links to plans, reports)\n\nB. Zoning &amp; Planning Permits\nGET /permits/planning\nGET /permits/zoning\nGET /permits/use\n\nRequired Fields:\n\nAll building permit fields (where applicable)\nzoning_district\nland_use_type\nceqa_determination (environmental review)\nvariance_requested\npublic_hearing_date\nplanning_commission_action\nappeal_period_end\n\nC. Business Licenses\nGET /licenses/business\nGET /licenses/business/{license_id}\n\nRequired Fields:\n\nlicense_id\nbusiness_name\ndba_name (doing business as)\nbusiness_type\nnaics_code (industry classification)\naddress\nowner_name\nissue_date\nexpiration_date\nstatus (active, expired, suspended)\nemployee_count\n\nD. Code Enforcement\nGET /enforcement/cases\nGET /enforcement/violations\n\nRequired Fields:\n\ncase_id\naddress\nviolation_type\nstatus\nopen_date\nclose_date\ncompliance_date\n\nE. Inspections\nGET /inspections\nGET /inspections?permit_id={id}\n\nRequired Fields:\n\ninspection_id\npermit_id (link to permit)\ninspection_type\nscheduled_date\ncompleted_date\nresult (passed, failed, conditional)\ninspector_notes\n\n\n2.3 Query Capabilities\nFiltering:\n?status=issued\n?issue_date_from=2024-01-01&amp;issue_date_to=2024-12-31\n?address=*Shattuck*\n?zoning_district=C-DMU\n?units_new&gt;=50\n\nPagination:\n?limit=1000\n?offset=5000\n?page=3&amp;per_page=500\n\nSorting:\n?sort=issue_date\n?sort=-valuation  (descending)\n\nField Selection:\n?fields=permit_id,address,units_new,issue_date\n\nAggregation:\n?aggregate=sum(units_new)&amp;group_by=year\n?aggregate=count(*)&amp;group_by=zoning_district\n\n\n2.4 Geospatial Data\nGeoJSON Support:\nGET /permits/building.geojson\n\nResponse:\n{\n  &quot;type&quot;: &quot;FeatureCollection&quot;,\n  &quot;features&quot;: [\n    {\n      &quot;type&quot;: &quot;Feature&quot;,\n      &quot;geometry&quot;: {\n        &quot;type&quot;: &quot;Point&quot;,\n        &quot;coordinates&quot;: [-122.2728, 37.8715]\n      },\n      &quot;properties&quot;: {\n        &quot;permit_id&quot;: &quot;...&quot;,\n        &quot;address&quot;: &quot;...&quot;,\n        ...\n      }\n    }\n  ]\n}\nBounding Box Queries:\n?bbox=-122.30,37.85,-122.23,37.92  (west,south,east,north)\n\nIntegration with Berkeley GIS:\n\nCross-reference to parcel data\nLink to zoning maps\nConnect to city infrastructure datasets\n\n\n3. SMART CITY DATA INTEGRATION\n3.1 Multi-Domain Analytics\nEnable cross-domain queries connecting:\nHousing + Economic Development:\nGET /analytics/housing-business-correlation\n\n\nNew housing units vs new business licenses\nResidential development impact on commercial corridors\n\nPublic Health + Building:\nGET /analytics/health-housing\n\n\nHousing conditions violations\nLead paint remediation permits\nADA compliance upgrades\n\nEnergy + Building:\nGET /analytics/energy-permits\n\n\nSolar installation permits\nEnergy efficiency upgrades\nEV charging infrastructure\n\nWaste + Building:\nGET /analytics/waste-construction\n\n\nConstruction debris tracking\nRecycling compliance\nHazardous material removal\n\nWater + Building:\nGET /analytics/water-permits\n\n\nPlumbing permits\nWater conservation measures\nGraywater systems\n\n\n3.2 UrbanSim Integration\nData Export Format:\nGET /export/urbansim\n\nProvide data in UrbanSim-compatible format:\n\nParcels table\nBuildings table\nHouseholds table\nJobs table\nDevelopment pipeline table\n\nSchema mapping documentation for:\n\nConverting Clariti permits to UrbanSim buildings\nAggregating to parcel level\nTime-series development tracking\n\n\n4. PERFORMANCE REQUIREMENTS\n4.1 Response Times\n\nSingle record: &lt; 200ms\nPaginated queries (1000 records): &lt; 2 seconds\nBulk exports (10,000+ records): &lt; 30 seconds\nGeospatial queries: &lt; 5 seconds\n\n4.2 Rate Limits\n\nPublic (no auth): 100 requests/hour\nRegistered users: 1,000 requests/hour\nAcademic/Research (.edu): 10,000 requests/hour\nCity partners: 100,000 requests/hour\n\n4.3 Availability\n\n99.5% uptime SLA\nPlanned maintenance windows announced 7 days in advance\nStatus page: status.clariti.berkeley.gov\n\n\n5. DATA QUALITY &amp; STANDARDS\n5.1 Address Standardization\n\nUSPS-standardized addresses\nGeocoding using NAD83 / EPSG:4326\nQuality flag for geocoding confidence\n\n5.2 Date Formats\n\nISO 8601: 2024-01-15T14:30:00-08:00\nConsistent timezone (Pacific)\n\n5.3 NULL Handling\n\nExplicit null in JSON (not empty strings)\nDocumentation of which fields can be null\n\n5.4 Data Dictionary\n\nComplete field documentation\nEnumerated values (e.g., all permit statuses)\nUnits of measurement clearly specified\n\n\n6. DOCUMENTATION REQUIREMENTS\n6.1 Public API Documentation\n\nInteractive documentation (Swagger/OpenAPI)\nCode examples in Python, R, JavaScript\nAuthentication guide\nRate limit explanations\nCommon query patterns\n\n6.2 Changelog\n\nVersion history\nBreaking changes clearly marked\nMigration guides for API updates\n\n6.3 Sample Data\n\nAnonymized/redacted sample datasets\nTest endpoints with synthetic data\nSandbox environment for development\n\n\n7. HISTORICAL DATA\n7.1 Accela Migration\n\nAll historical Accela data accessible via Clariti API\nMinimum 10 years of history\nClearly marked legacy vs new data\nField mapping documentation (Accela ‚Üí Clariti)\n\n7.2 Data Retention\n\nPermits: Permanent retention\nInspections: 10 years minimum\nCode violations: 7 years minimum\n\n\n8. PRIVACY &amp; REDACTION\n8.1 Public Records Act Compliance\n\nApplicant/owner names: public\nPersonal phone/email: redacted\nFinancial details beyond valuation: redacted\n\n8.2 Sensitive Data Handling\n\nDV SafeAtHome addresses: redacted\nOngoing investigations: delayed publication\n\n\n9. DEVELOPER SUPPORT\n9.1 Getting Started\n\nSelf-service app token registration\nQuick start guide\nJupyter notebook examples\nR package/Python library\n\n9.2 Support Channels\n\nDeveloper forum/mailing list\nGitHub repository for community scripts\nIssue tracker for API bugs\nQuarterly developer office hours\n\n\n10. REFERENCE IMPLEMENTATIONS\n10.1 Example Use Cases\nProvide working examples for:\n\nHousing pipeline analysis\nEconomic development tracking\nPublic health research\nEnergy efficiency monitoring\nTransportation impact analysis\n\n10.2 Open Source Tools\n\nPython SDK: pip install clariti-berkeley\nR package: install.packages(&quot;claritiR&quot;)\nCommand-line tool: clariti-cli\n\n\n11. COMPARISON TO BEST PRACTICES\n11.1 Benchmark Systems\n\nSocrata (used by many cities)\nOpenGov (transparency focus)\nSan Francisco DataSF\nChicago Data Portal\nNYC Open Data\n\n11.2 Standards Compliance\n\nOpen311 (for service requests)\nBLDS (Building and Land Development Specification)\nPDDL (Public Domain Dedication License) for data\n\n\n12. SUCCESS METRICS\n12.1 Adoption Metrics\n\nNumber of registered API users\nAPI call volume\nNumber of derived applications\nAcademic papers using the data\n\n12.2 Quality Metrics\n\nAPI uptime\nAverage response time\nError rate\nUser satisfaction surveys\n\n\n13. FUTURE ENHANCEMENTS\n13.1 Phase 2 Capabilities\n\nReal-time permit status webhooks\nMachine learning APIs (permit timeline prediction)\n3D building model exports\nAR/VR integration endpoints\n\n13.2 Advanced Analytics\n\nTime-series forecasting endpoints\nAnomaly detection APIs\nComparative analytics (Berkeley vs peer cities)\n\n\nAPPENDIX A: Contact Information\nCity of Berkeley:\n\nIT Department: [contact]\nPlanning Department: [contact]\nBuilding Department: [contact]\n\nClariti:\n\nImplementation team: [contact]\nAPI support: [contact]\n\nResearch Partners:\n\nUC Berkeley: [contact]\nCommunity organizations: [contact]\n\n\nAPPENDIX B: Example API Calls\nGet all permits issued in 2024\ncurl &quot;api.clariti.berkeley.gov/v1/permits/building\nGet large residential projects\ncurl &quot;api.clariti.berkeley.gov/v1/permits/building\nGet permits on a specific street\ncurl &quot;api.clariti.berkeley.gov/v1/permits/building*Shattuck*&amp;sort=-issue_date&quot;\nGet geospatial data\ncurl &quot;api.clariti.berkeley.gov/v1/permits/building.geojson,37.85,-122.23,37.92&quot;\n\nDocument Version: 1.0\nLast Updated: 2026-01-14\nNext Review: Upon Clariti implementation kickoff"},"research/clariti/research_questions":{"slug":"research/clariti/research_questions","filePath":"research/clariti/research_questions.md","title":"research_questions","links":[],"tags":[],"content":"Clariti Research Questions\nAbout Clariti\n\nWhat is Clariti‚Äôs full product suite?\nWhat cities currently use Clariti?\nWhat API capabilities do they offer?\nDo they have public API documentation?\nWhat data formats do they support?\n\nCurrent Implementations\n\nWhich cities have public-facing Clariti APIs?\nAre there example API endpoints we can reference?\nWhat‚Äôs the data model/schema?\nWhat are best practices from existing implementations?\n\nBerkeley-Specific\n\nWhen is Berkeley migrating from Accela to Clariti?\nWhat‚Äôs the migration timeline?\nWill historical Accela data be accessible via Clariti?\nWho is the Berkeley contact for this project?\n\nComparison to Other Platforms\n\nHow does Clariti compare to Accela?\nHow does it compare to other permit systems?\nWhat about integration with UrbanSim?\n"}}